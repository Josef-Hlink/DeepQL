{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Random Warmup\n",
    "\n",
    "In this notebook, we explore the results generated by the `2_randomwarmup.sh` script, where we reduce the number of total episodes from 50k (see `1_bruteforce.ipynb`) to 20k and make the agents start with a random policy 10k _steps_ before their actual training starts.\n",
    "This way, their can fill up their buffer(s) with random (i.e., not biased) experiences before they start learning from them.\n",
    "\n",
    "The idea is that this should help the agents overcome the initial bias they seem to be suffering from in the first $\\pm$ 20k episodes.\n",
    "If we see any improvements to their rewards and/or action biases, we can infer that using a random warmup is a good idea.\n",
    "\n",
    "The configurations that we explore are largely similar to the brute force experiments.\n",
    "The only difference being that for the agents using TN, the target network update frequency is reduced from 2.5k to 1k.\n",
    "This way, the target network is now updated just as many times as in the brute force experiments.\n",
    "\n",
    "The other settings are also pretty much the same, but with these adjustments:\n",
    "\n",
    "| parameter                     | previous value | new value\n",
    "|-------------------------------|----------------|----------\n",
    "| number of individual runs     | 6              | 5\n",
    "| number of episodes            | 50k            | 20k\n",
    "| number of random warmup steps | 0              | 10k\n",
    "\n",
    "In addition to the above, we also explore the difference between two distinct annealing schemes for the $\\varepsilon$-greedy exploration strategy.\n",
    "\n",
    "The first one is the one we used in the brute force experiments, where the $\\varepsilon$ value is exponentially annealed from 1 to 0.01 over the course of the first 80% of the total number of episodes.\n",
    "The second one is a slightly less aggressive annealing scheme, where the $\\varepsilon$ value is annealed linearly from 1 to 0.1 over the course of the first 50% of the total number of episodes.\n",
    "\n",
    "| scheme | from | to   | window | kind\n",
    "|--------|------|------|--------|------\n",
    "| 0      | 1    | 1    | 0      | -\n",
    "| 1      | 1    | 0.01 | 80%    | exponential\n",
    "| 2*     | 1    | 0.01 | 80%    | linear\n",
    "| 3*     | 1    | 0.1  | 50%    | exponential\n",
    "| 4      | 1    | 0.1  | 50%    | linear\n",
    "\n",
    "\\* Schemes 2 and 3 are not examined in this project, but are included in the `agent.annealing` module for possible later exploration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dql.utils.namespaces import P\n",
    "from dql.utils.datamanager import ConcatDataManager\n",
    "from dql.utils.plotter import ColorPlot, LossPlot, ComparisonPlot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have the data.\n",
    "\n",
    "Should be BL, ER, TN, and TR for both annealing schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runIDs = [f for f in os.listdir(P.data) if f.startswith('AA')]\n",
    "print('\\n'.join(runIDs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the parameters are correct.\n",
    "We check for the run using the `TR` config, since it will contain all the hyperparameters.\n",
    "For the first annealing scheme, we print the full summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConcatDataManager('AA1-TR').printSummary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AA4` run only differs in annealing scheme, so we load and print this separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ConcatDataManager('AA4-TR').loadSummary().params.annealingScheme.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to easily get all figures for a given run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNames = {'BL': 'Baseline', 'ER': 'Experience Replay', 'TN': 'Target Network', 'TR': 'Target Network + Experience Replay'}\n",
    "\n",
    "def getFigs(runID: str, exp: int) -> tuple[plt.Figure]:\n",
    "    expID, epxName = ('AA1', 'Annealing Scheme 1') if exp == 1 else ('AA4', 'Annealing Scheme 4')\n",
    "    \n",
    "    title = f'| {runNames[runID]}\\n({epxName})'\n",
    "    DM = ConcatDataManager(f'{expID}-{runID}')\n",
    "\n",
    "    R = DM.loadRewards()\n",
    "    fR = ColorPlot(R, label='reward', title=title).getFig()\n",
    "\n",
    "    A = DM.loadActions()\n",
    "    AB = np.abs((A / np.sum(A, axis=2, keepdims=True))[:, :, 0] - .5) * 2\n",
    "    fAB = ColorPlot(AB, label='action bias', title=title).getFig()\n",
    "\n",
    "    L = DM.loadLosses()\n",
    "    fL = LossPlot(L, title=title).getFig()\n",
    "    return fR, fAB, fL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runID = 'BL'\n",
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 1)\n",
    "rewardFig.savefig(Path(P.plots) / f'AA1-{runID}-R.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runID = 'ER'\n",
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runID = 'TN'\n",
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Target Network + Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runID = 'TR'\n",
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardFig, actionBiasFig, lossFig = getFigs(runID, 4)\n",
    "rewardFig.savefig(Path(P.plots) / f'AA4-{runID}-R.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "data4 = []\n",
    "# redefine runIDs to get the correct order\n",
    "runIDs = ['BL', 'ER', 'TN', 'TR']\n",
    "for runID in runIDs:\n",
    "    DM1 = ConcatDataManager(f'AA1-{runID}')\n",
    "    DM4 = ConcatDataManager(f'AA4-{runID}')\n",
    "    R1, R4 = DM1.loadRewards(), DM4.loadRewards()\n",
    "    A1, A4 = DM1.loadActions(), DM4.loadActions()\n",
    "    AB1 = np.abs((A1 / np.sum(A1, axis=2, keepdims=True))[:, :, 0] - .5) * 2\n",
    "    AB4 = np.abs((A4 / np.sum(A4, axis=2, keepdims=True))[:, :, 0] - .5) * 2\n",
    "    data1.append((R1, AB1))\n",
    "    data4.append((R4, AB4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = ComparisonPlot(data1, runIDs, 'Annealing Scheme 1').getFig()\n",
    "fig1.savefig(Path(P.plots) / 'AA1-C.png', dpi=500, bbox_inches='tight')\n",
    "fig4 = ComparisonPlot(data4, runIDs, 'Annealing Scheme 4').getFig()\n",
    "fig4.savefig(Path(P.plots) / 'AA4-C.png', dpi=500, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
